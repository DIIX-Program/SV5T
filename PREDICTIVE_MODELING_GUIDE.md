# PREDICTIVE ANALYTICS & RECOMMENDATION ENGINE

**HÆ°á»›ng dáº«n triá»ƒn khai mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n & há»‡ thá»‘ng gá»£i Ã½ cho SV5T**

---

## 1. OVERVIEW: CÃC BÆ¯á»šC PHÃT TRIá»‚N MÃ” HÃŒNH Dá»° ÄOÃN

```
Phase 1: Data Collection (Hiá»‡n táº¡i)
  â””â”€ Sinh viÃªn nháº­p thÃ´ng tin â†’ LÆ°u DB
  â””â”€ Output: Dataset records vá»›i 25+ features

Phase 2: Exploratory Analysis (ÄÃ£ lÃ m)
  â”œâ”€ Descriptive statistics
  â”œâ”€ Correlation analysis
  â”œâ”€ Clustering & segmentation
  â””â”€ Output: Insights tá»« dá»¯ liá»‡u hiá»‡n cÃ³

Phase 3: Predictive Modeling (Äá» xuáº¥t)
  â”œâ”€ Chá»n model (Logistic Regression, Random Forest, ...)
  â”œâ”€ Train/Validation/Test split
  â”œâ”€ Hyperparameter tuning
  â”œâ”€ Cross-validation
  â””â”€ Output: Model vá»›i accuracy ~88-92%

Phase 4: Deployment (Äá» xuáº¥t)
  â”œâ”€ REST API: /api/predict
  â”œâ”€ Real-time inference
  â”œâ”€ Caching strategy
  â””â”€ Output: Predictions trong <500ms

Phase 5: Monitoring & Retraining (Äá» xuáº¥t)
  â”œâ”€ Collect predictions vs actual outcomes
  â”œâ”€ Detect model drift
  â”œâ”€ Retrain monthly/quarterly
  â””â”€ Output: Model performance tracking
```

---

## 2. BÆ¯á»šC 1: DATA PREPARATION

### 2.1 Táº­p Dá»¯ Liá»‡u ÄÃ o Táº¡o (Training Set)

**Thá»i gian**: 2022-2023 (Ã­t nháº¥t 200-300 sinh viÃªn)

```
Input Features (20-25):
â”œâ”€ Demographics: faculty, student_type, academic_year
â”œâ”€ Hard Criteria: hard_ethics, hard_study, hard_physical, ...
â”œâ”€ Soft Criteria: soft_ethics_score, soft_study_score, ...
â”œâ”€ Profile: gpa, training_points, volunteer_days
â”œâ”€ Engagement: evidences_count, evidence_approval_rate
â””â”€ Temporal: submission_timeline_days, last_update_recency

Output Label (Target):
â”œâ”€ Classification: final_status (ELIGIBLE / ALMOST_READY / NOT_ELIGIBLE)
â””â”€ Regression: completion_percent (0-100)
```

### 2.2 Data Splitting

```
Total: N students

Training: 70% â†’ Ä‘á»ƒ train model
Validation: 15% â†’ Ä‘á»ƒ tune hyperparameters
Test: 15% â†’ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ final performance

IMPORTANT: Split by TIME (khÃ´ng random)
â”œâ”€ Train: Jan-Aug 2023
â”œâ”€ Val:   Sep 2023
â””â”€ Test:  Oct-Dec 2023 + Jan 2024

LÃ½ do: TrÃ¡nh data leakage (future info)
```

### 2.3 Xá»­ LÃ½ Dá»¯ Liá»‡u Thiáº¿u & Ngoáº¡i Lá»‡

```
Missing Values:
â”œâ”€ submission_timeline_days: Fill with median (40 days)
â”œâ”€ last_update_recency: Fill with 999 (khÃ´ng update)
â””â”€ evidence_approval_rate: Fill with 0 (khÃ´ng cÃ³ minh chá»©ng)

Outliers:
â”œâ”€ GPA = 0 (invalid) â†’ Impute vá»›i group mean
â”œâ”€ Volunteer = 365 days (unrealistic) â†’ Cap at 100 days
â”œâ”€ Training points > 100 â†’ Cap at 100
â””â”€ Detect & Flag: Zscore > 3 â†’ Review manually

Class Imbalance (náº¿u cÃ³):
â”œâ”€ ELIGIBLE: 45%, ALMOST_READY: 30%, NOT_ELIGIBLE: 25%
â”œâ”€ Strategy: SMOTE hoáº·c Class weights
â””â”€ Metric: F1-score thay cho Accuracy
```

---

## 3. BÆ¯á»šC 2: FEATURE ENGINEERING

### 3.1 Feature Selection

**Theo importance (tá»« EDA):**

```
Top Features to Use:
1. hard_study â˜…â˜…â˜…â˜…â˜… (Weight: 25%)
2. soft_integration_score â˜…â˜…â˜…â˜… (Weight: 15%)
3. hard_volunteer â˜…â˜…â˜…â˜… (Weight: 12%)
4. gpa â˜…â˜…â˜… (Weight: 11%)
5. soft_study_score â˜…â˜…â˜… (Weight: 8%)
6. training_points â˜…â˜…â˜… (Weight: 7%)
7. hard_ethics â˜…â˜… (Weight: 6%)
8. volunteer_days â˜…â˜… (Weight: 5%)
9. evidence_approval_rate â˜…â˜… (Weight: 5%)
10. submission_recency â˜…â˜… (Weight: 5%)

Skip:
âŒ hard_physical (quÃ¡ khÃ³, correlation tháº¥p)
âŒ student_id, mssv (khÃ´ng cÃ³ predictive power)
âŒ CÃ³ thá»ƒ bá» soft_physical_score (luÃ´n = 0)
```

### 3.2 Feature Transformation

```
Numerical Features:
â”œâ”€ Standardization (Z-score normalization):
â”‚  â”œâ”€ GPA: (x - 3.4) / 0.3 â†’ mean = 0, std = 1
â”‚  â”œâ”€ Training: (x - 90) / 8
â”‚  â””â”€ Volunteer: log(x + 1) â†’ handle right-skew
â”‚
â”œâ”€ Scaling:
â”‚  â”œâ”€ MinMax scaling cho tree models: khÃ´ng cáº§n
â”‚  â””â”€ StandardScaler cho linear models: cáº§n
â”‚
â””â”€ Binning (Optional):
   â””â”€ GPA â†’ 4 bins: <3.0, 3.0-3.4, 3.4-3.7, â‰¥3.7

Categorical Features:
â”œâ”€ Faculty: One-hot encoding
â”‚  â”œâ”€ faculty_kinh_te: 0/1
â”‚  â”œâ”€ faculty_ky_thuat: 0/1
â”‚  â”œâ”€ faculty_y_khoa: 0/1
â”‚  â””â”€ faculty_other: dropped (reference)
â”‚
â””â”€ Student Type: Label encoding
   â””â”€ student_type_university: 0/1

Interaction Features (Optional):
â”œâ”€ gpa Ã— training (Ethical student who studies)
â”œâ”€ volunteer_days Ã— soft_integration_score
â””â”€ hard_passed_count Ã— soft_total_score
```

### 3.3 Feature Normalization

```
TrÆ°á»›c Training:
â”œâ”€ Standardize: (x - Î¼) / Ïƒ
â”œâ”€ Fit scaler trÃªn TRAINING data
â”œâ”€ Apply same scaler tá»›i VAL & TEST data
â”‚  (IMPORTANT: TrÃ¡nh data leakage)
â””â”€ Save scaler â†’ dÃ¹ng láº¡i khi dá»± Ä‘oÃ¡n new students

Code Pattern:
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit + Transform
X_val_scaled = scaler.transform(X_val)          # Transform only
X_test_scaled = scaler.transform(X_test)        # Transform only

# Later, for new student:
new_student_scaled = scaler.transform(new_student)
prediction = model.predict(new_student_scaled)
```
```

---

## 4. BÆ¯á»šC 3: MODEL SELECTION & TRAINING

### 4.1 Candidate Models

```
1. LOGISTIC REGRESSION (Baseline)
   â”œâ”€ Pros: Simple, interpretable, fast
   â”œâ”€ Cons: Assumes linearity
   â”œâ”€ Time to train: < 1 second
   â”œâ”€ Expected Accuracy: 80-85%
   â””â”€ Use when: Need fast inference & explainability

2. RANDOM FOREST (Recommended)
   â”œâ”€ Pros: Non-linear, robust, feature importance
   â”œâ”€ Cons: Slower inference, black box
   â”œâ”€ Time to train: 10-30 seconds (100 trees)
   â”œâ”€ Expected Accuracy: 88-92%
   â””â”€ Use when: Want better performance + feature importance

3. GRADIENT BOOSTING (XGBoost/LightGBM)
   â”œâ”€ Pros: Best performance, handles imbalance well
   â”œâ”€ Cons: Complex, risk of overfitting
   â”œâ”€ Time to train: 30-60 seconds
   â”œâ”€ Expected Accuracy: 90-94%
   â””â”€ Use when: Max performance is priority

4. NEURAL NETWORK (Deep Learning)
   â”œâ”€ Pros: Very flexible, can learn complex patterns
   â”œâ”€ Cons: Needs more data, slow training, black box
   â”œâ”€ Time to train: 1-5 minutes
   â”œâ”€ Expected Accuracy: 91-95%
   â””â”€ Use when: Have 1000+ samples & computation budget

Recommended: Random Forest (sweet spot)
```

### 4.2 Hyperparameter Tuning (Random Forest)

```
Parameters to Tune:

n_estimators: [50, 100, 150, 200]
â”œâ”€ More trees â†’ better, nhÆ°ng cháº­m hÆ¡n
â””â”€ Recommended: 100

max_depth: [5, 10, 15, None]
â”œâ”€ Deeper tree â†’ cÃ³ thá»ƒ overfit
â””â”€ Recommended: 10

min_samples_split: [2, 5, 10]
â”œâ”€ Cao hÆ¡n â†’ simpler model, Ã­t overfit
â””â”€ Recommended: 5

min_samples_leaf: [1, 2, 4]
â”œâ”€ Cao hÆ¡n â†’ smoother predictions
â””â”€ Recommended: 2

max_features: ['sqrt', 'log2']
â”œâ”€ Bao nhiÃªu features xem á»Ÿ má»—i split
â””â”€ Recommended: 'sqrt'

Tuning Strategy:
â”œâ”€ Grid Search: Thá»­ táº¥t cáº£ combinations (lÃ¢u)
â”œâ”€ Random Search: Random sample (nhanh hÆ¡n)
â””â”€ Bayesian Optimization: Smart search (tá»‘t nháº¥t)

Validation: 5-fold Cross-Validation
```

---

## 5. BÆ¯á»šC 4: MODEL EVALUATION

### 5.1 Classification Metrics

```
Confusion Matrix:
                 Predicted
              Eligible  Other
Actual  Eligible   TP      FN
        Other      FP      TN

Key Metrics:

1. ACCURACY = (TP + TN) / Total
   â””â”€ Tá»•ng % dá»± Ä‘oÃ¡n Ä‘Ãºng
   â””â”€ Problem: KhÃ´ng tá»‘t khi imbalanced classes

2. PRECISION = TP / (TP + FP)
   â””â”€ Cá»§a nhá»¯ng dá»± Ä‘oÃ¡n "Eligible", % Ä‘Ãºng bao nhiÃªu?
   â””â”€ High precision = Ã­t false positives
   â””â”€ Important: ChÃºng ta khÃ´ng muá»‘n report sai

3. RECALL = TP / (TP + FN)
   â””â”€ Cá»§a nhá»¯ng sinh viÃªn thá»±c táº¿ "Eligible", % Ä‘Æ°á»£c dá»± Ä‘oÃ¡n?
   â””â”€ High recall = Ã­t false negatives
   â””â”€ Important: KhÃ´ng muá»‘n bá» sÃ³t

4. F1-SCORE = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
   â””â”€ Trung bÃ¬nh hÃ²a cá»§a P & R
   â””â”€ Tá»‘t khi classes imbalanced

5. ROC-AUC = Area under ROC Curve
   â””â”€ Äo kháº£ nÄƒng phÃ¢n biá»‡t giá»¯a classes
   â””â”€ 0.5 = random, 1.0 = perfect
   â””â”€ Target: â‰¥ 0.90

Example Results:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model: Random Forest                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Accuracy:  90.5%                    â”‚
â”‚ Precision: 89.2%                    â”‚
â”‚ Recall:    88.7%                    â”‚
â”‚ F1-Score:  88.9%                    â”‚
â”‚ ROC-AUC:   0.923                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 Per-Class Performance

```
Class-wise Metrics:

                Precision  Recall   F1-Score  Support
ELIGIBLE         92%        90%      91%       45
ALMOST_READY     85%        82%      84%       30
NOT_ELIGIBLE     88%        91%      90%       25

Interpretation:
â”œâ”€ ELIGIBLE: Model tá»‘t (high P & R)
â”œâ”€ ALMOST_READY: Model yáº¿u (recall tháº¥p â†’ bá» sÃ³t)
â””â”€ NOT_ELIGIBLE: Model tá»‘t

Action: CÃ¢n nháº¯c giáº£m threshold Ä‘á»ƒ tÄƒng recall cho ALMOST_READY
```

### 5.3 Feature Importance

```
Feature Importance (Random Forest):

1. hard_study              25.3%  â˜…â˜…â˜…â˜…â˜…
2. soft_integration_score  14.8%  â˜…â˜…â˜…â˜…
3. gpa                     12.1%  â˜…â˜…â˜…
4. hard_volunteer          11.5%  â˜…â˜…â˜…
5. soft_study_score        10.2%  â˜…â˜…â˜…
6. training_points          8.4%  â˜…â˜…
7. hard_ethics              7.2%  â˜…â˜…
8. volunteer_days           5.8%  â˜…
9. evidence_approval_rate   3.2%  â˜…
10. submission_recency      1.4%  

Top 3: hard_study + soft_integration + gpa = 52%
â†’ Táº­p trung vÃ o ba yáº¿u tá»‘ nÃ y â†’ cáº£i thiá»‡n 50% outcome
```

---

## 6. BÆ¯á»šC 5: DEPLOYMENT & INFERENCE

### 6.1 Model Serving Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          PREDICTION PIPELINE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚ 1. Web Form Input                               â”‚
â”‚    â””â”€ Student data from React component         â”‚
â”‚                                                 â”‚
â”‚ 2. Data Validation                              â”‚
â”‚    â””â”€ Check GPA range, training points, etc.    â”‚
â”‚                                                 â”‚
â”‚ 3. Feature Engineering                          â”‚
â”‚    â””â”€ Create derived features                   â”‚
â”‚                                                 â”‚
â”‚ 4. Feature Scaling                              â”‚
â”‚    â””â”€ Apply saved StandardScaler                â”‚
â”‚                                                 â”‚
â”‚ 5. Model Prediction                             â”‚
â”‚    â””â”€ Load trained model â†’ predict              â”‚
â”‚                                                 â”‚
â”‚ 6. Post-Processing                              â”‚
â”‚    â””â”€ Add confidence, recommendations           â”‚
â”‚                                                 â”‚
â”‚ 7. Cache Result                                 â”‚
â”‚    â””â”€ Store in localStorage (24 hours)          â”‚
â”‚                                                 â”‚
â”‚ 8. Return JSON                                  â”‚
â”‚    â””â”€ Status + probability + confidence         â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 API Endpoint Design

```
POST /api/predict
â”œâ”€ Request:
â”‚  {
â”‚    "gpa": 3.5,
â”‚    "training_points": 92,
â”‚    "volunteer_days": 6,
â”‚    "hard_criteria": [1, 1, 0, 1, 1],
â”‚    "soft_criteria": [3, 3, 0, 3, 4],
â”‚    "faculty": "kinh_te",
â”‚    "student_type": "UNIVERSITY"
â”‚  }
â”‚
â””â”€ Response:
   {
     "student_id": "123456",
     "predicted_status": "ELIGIBLE",
     "probabilities": {
       "ELIGIBLE": 0.82,
       "ALMOST_READY": 0.15,
       "NOT_ELIGIBLE": 0.03
     },
     "confidence_score": 0.91,
     "readiness_score_predicted": 78.5,
     "improvement_needs": [
       {
         "criteria": "Physical",
         "urgency": "MEDIUM",
         "estimated_days": 60
       }
     ],
     "success_probability": 0.82,
     "success_if_improved": 0.94,
     "inference_time_ms": 45
   }
```

### 6.3 Caching Strategy

```
Client-side Caching:
â”œâ”€ Cache key: student_id + current_date
â”œâ”€ Cache duration: 24 hours
â”œâ”€ Invalidate when: Form data changed > 10%
â””â”€ Benefit: <100ms response time

Server-side Caching:
â”œâ”€ Cache layer: Redis (optional)
â”œâ”€ TTL: 6 hours
â”œâ”€ Eviction: LRU (Least Recently Used)
â””â”€ Benefit: Reduce model inference load

Invalidation Triggers:
â”œâ”€ Manual: User clicks "Recalculate"
â”œâ”€ Automatic: 24 hours passed
â”œâ”€ On data change: Major form update
â””â”€ On model update: New model deployed
```

---

## 7. BÆ¯á»šC 6: IMPROVEMENT RECOMMENDATION ENGINE

### 7.1 Algorithm: What to Improve?

```
Algorithm prioritizeImprovements(student_profile):
    
    improvements = []
    
    for each criteria in [study, volunteer, ethics, physical, integration]:
        
        if not student.hard_pass[criteria]:
            
            // 1. Calculate deficit
            deficit = threshold[criteria] - current[criteria]
            
            // 2. Look at similar students who improved
            improved_cohort = filter(
                historical_data,
                criteria: criteria,
                before_status: FAILED,
                after_status: PASSED
            )
            
            // 3. Estimate effort needed
            effort_days = analyze_timeline(improved_cohort)
            effort_percent = analyze_intensity(improved_cohort)
            
            // 4. Assess urgency
            if criteria == 'study':
                urgency = 'HIGH'          // Hardest to improve
                recommend_urgently = true
            elif criteria in ['volunteer', 'ethics']:
                urgency = 'MEDIUM'
                recommend_urgently = false
            else:
                urgency = 'LOW'
            
            // 5. Attach evidence
            success_rate = len(improved_cohort) / len(total_cohort)
            
            improvements.append({
                criteria: criteria,
                deficit: deficit,
                urgency: urgency,
                estimated_days: effort_days,
                success_rate: success_rate,
                related_events: recommendEvents(criteria)
            })
    
    return improvements.sortBy('urgency', 'deficit')
```

### 7.2 Event Recommendation System

```
Algorithm: Score Events for Student

for each event in available_events:
    
    relevance_score = 0
    
    // 1. Relevance to needed criteria
    for criteria in event.supported_categories:
        if student.hard_pass[criteria] == FALSE:
            relevance_score += 40    // Very relevant
        elif student.soft[criteria] < 6:
            relevance_score += 20    // Somewhat relevant
        else:
            relevance_score += 5     // Nice to have
    
    // 2. Timing
    days_until = (event.date - today).days
    if days_until in [0, 30]:
        time_score = 40    // Urgent
    elif days_until in (30, 90]:
        time_score = 25    // Soon
    else:
        time_score = 5     // Later
    
    // 3. Historical success rate
    similar_students_who_attended = filter(
        historical_data,
        attended_event: event.id,
        similar_profile: student
    )
    success_rate = len(improved) / len(attended)
    success_score = success_rate * 20
    
    // 4. Combine scores
    event.recommendation_score = (
        0.5 Ã— relevance_score +
        0.3 Ã— time_score +
        0.2 Ã— success_score
    )

// Return top 3 events
return events.sortBy('recommendation_score').take(3)
```

Example Output:

```
Top Recommended Events for Student A:

1. ğŸ¯ KhÃ³a cáº£i thiá»‡n GPA (Score: 95/100)
   â”œâ”€ Cáº§n thiáº¿t cho: Study Hard (GPA cáº§n +0.3)
   â”œâ”€ Thá»i gian: 15 thÃ¡ng 2 nÄƒm 2025 (10 ngÃ y ná»¯a)
   â”œâ”€ ThÃ nh cÃ´ng rate: 78% (tá»« 45 sinh viÃªn tÆ°Æ¡ng tá»±)
   â”œâ”€ Æ¯á»›c tÃ­nh thá»i gian: 8 tuáº§n
   â””â”€ [ÄÄƒng kÃ½] [Chi tiáº¿t]

2. ğŸƒ Há»™i thao - Kiá»ƒm tra SV Khá»e (Score: 82/100)
   â”œâ”€ Cáº§n thiáº¿t cho: Physical Hard
   â”œâ”€ Thá»i gian: 1 thÃ¡ng 6 nÄƒm 2025
   â”œâ”€ ThÃ nh cÃ´ng rate: 92%
   â”œâ”€ Æ¯á»›c tÃ­nh thá»i gian: 2 thÃ¡ng training
   â””â”€ [ÄÄƒng kÃ½] [Chi tiáº¿t]

3. ğŸ¤ Chiáº¿n dá»‹ch TÃ¬nh nguyá»‡n HÃ¨ (Score: 76/100)
   â”œâ”€ Cáº§n thiáº¿t cho: Volunteer Hard + Soft
   â”œâ”€ Thá»i gian: 1 thÃ¡ng 7 nÄƒm 2025
   â”œâ”€ ThÃ nh cÃ´ng rate: 95%
   â”œâ”€ Æ¯á»›c tÃ­nh thá»i gian: 3 tuáº§n
   â””â”€ [ÄÄƒng kÃ½] [Chi tiáº¿t]
```

---

## 8. BÆ¯á»šC 7: MONITORING & RETRAINING

### 8.1 Model Performance Monitoring

```
Metrics to Track:

1. Prediction Accuracy (Monthly)
   â”œâ”€ Actual outcomes vs predicted
   â”œâ”€ Alert threshold: Accuracy drops < 85%
   â””â”€ Action: Investigate root cause or retrain

2. Model Drift Detection
   â”œâ”€ Feature distribution change (Kolmogorov-Smirnov test)
   â”œâ”€ Prediction distribution change
   â”œâ”€ Alert: p-value < 0.05 (significant drift)
   â””â”€ Action: Retrain or update thresholds

3. Calibration
   â”œâ”€ P(Eligible) = 0.8 â†’ Actual eligible rate â‰ˆ 80%
   â”œâ”€ Check: Brier score, Expected Calibration Error
   â””â”€ Alert: Significant miscalibration
   â””â”€ Action: Apply calibration (Platt scaling, etc.)

4. Fairness Metrics
   â”œâ”€ Accuracy per faculty (should be similar)
   â”œâ”€ Accuracy per student type
   â”œâ”€ Alert: Bias detected (difference > 5%)
   â””â”€ Action: Retrain with fairness constraints

5. Latency
   â”œâ”€ Inference time (should be < 500ms)
   â”œâ”€ P95 latency (95% of requests)
   â””â”€ Alert: > 1000ms
   â””â”€ Action: Optimize or use faster model
```

### 8.2 Retraining Schedule

```
Trigger-based Retraining:
â”œâ”€ Immediate: Accuracy drops below 80% (emergency)
â”œâ”€ Weekly: Collect new data, check drift
â”œâ”€ Monthly: Full retraining with recent data
â”œâ”€ Quarterly: Major review, ablation studies
â””â”€ Yearly: Complete overhaul, new features

Retraining Pipeline:

1. Data Collection (Recent 3 months)
   â””â”€ New students + validation of predictions

2. Validation (Against test set)
   â””â”€ Ensure old predictions still hold

3. Model Training
   â””â”€ Retrain on historical + new data

4. Evaluation
   â””â”€ Validate on hold-out test set

5. Comparison
   â”œâ”€ New model vs old model
   â”œâ”€ Is performance better?
   â””â”€ Any fairness issues?

6. Deployment Decision
   â”œâ”€ If better: Deploy immediately
   â”œâ”€ If worse: Investigate & debug
   â””â”€ If same: Keep current (avoid churn)

7. Monitoring
   â””â”€ Track new model performance closely
```

### 8.3 Feedback Loop

```
Collecting Ground Truth:

When to collect:
â”œâ”€ End of year: After final decision made
â”œâ”€ Interview: Admin confirms eligibility
â”œâ”€ Appeal: Student challenges decision
â””â”€ Academic year end: Comprehensive review

Data to collect:
â”œâ”€ actual_status (what really happened)
â”œâ”€ confidence_score (original prediction)
â”œâ”€ improvement_notes (what student actually did)
â””â”€ feedback (student satisfaction with recommendations)

Using Feedback:

1. Calculate prediction error
   â””â”€ error = predicted_status â‰  actual_status

2. Analyze error patterns
   â”œâ”€ Which students did we get wrong?
   â”œâ”€ What was common? (low GPA, late submission, etc.)
   â””â”€ What did we miss?

3. Identify systematic biases
   â”œâ”€ Over-predicting certain faculty?
   â”œâ”€ Under-predicting certain student type?
   â””â”€ Seasonal patterns?

4. Update model accordingly
   â””â”€ Add features, adjust weights, or retrain
```

---

## 9. CONFIDENCE CALIBRATION

### 9.1 Why Confidence Matters

```
Prediction: "Student A â†’ ELIGIBLE (confidence: 82%)"
Meaning: "Dá»±a trÃªn profile, 82 láº§n ná»™p há»“ sÆ¡ giá»‘ng váº­y,
         khoáº£ng 82 láº§n Ä‘Æ°á»£c chá»n."

Uses:
â”œâ”€ Low confidence (< 60%): Flag for manual review
â”œâ”€ Medium confidence (60-80%): Show with caveats
â”œâ”€ High confidence (> 80%): Trust the prediction

How to Calculate Confidence:

1. From Similar Students (KNN-based)
   â””â”€ Find 10-20 similar students
   â””â”€ Count how many achieved target status
   â””â”€ confidence = (# achieved) / (total similar)

2. From Prediction Probability
   â””â”€ model.predict_proba() gives P(class)
   â””â”€ confidence = max(P(ELIGIBLE), P(ALMOST_READY), P(NOT_ELIGIBLE))
   â””â”€ Too high probability = overfitting (recalibrate)

3. From Model Uncertainty
   â””â”€ Tree-based: Entropy in leaf node
   â””â”€ NN: MC Dropout
   â””â”€ Ensemble: Variance across models

Recommendation:
â”œâ”€ Use combination of 1 + 2
â”œâ”€ Calibrate using Platt scaling
â””â”€ Validate on separate holdout set
```

### 9.2 Confidence Bands

```
Display in UI:

Very Confident (> 85%)
â”œâ”€ ğŸ“ˆ Dá»± Ä‘oÃ¡n nÃ y ráº¥t tin cáº­y (88% sinh viÃªn tÆ°Æ¡ng tá»± â†’ Ä‘áº¡t)
â”œâ”€ Color: Green âœ“
â””â”€ Action: Trust the prediction

Moderately Confident (65-85%)
â”œâ”€ ğŸ“Š Dá»± Ä‘oÃ¡n nÃ y khÃ¡ tin cáº­y (78% sinh viÃªn tÆ°Æ¡ng tá»± â†’ Ä‘áº¡t)
â”œâ”€ Color: Orange âš ï¸
â””â”€ Action: Consider alternatives, get more evidence

Low Confidence (< 65%)
â”œâ”€ â“ Dá»± Ä‘oÃ¡n khÃ´ng cháº¯c cháº¯n (55% sinh viÃªn tÆ°Æ¡ng tá»± â†’ Ä‘áº¡t)
â”œâ”€ Color: Red âœ—
â””â”€ Action: Need manual review or more information
```

---

## 10. CHUYÃŠN Má»¤C: CÃ”NG THá»¨C TÃNH TOÃN CHI TIáº¾T

### 10.1 Readiness Score (Data-Driven Version)

```
readinessScore(student, faculty) = 

    // Component 1: Hard Criteria (CÆ¡ báº£n)
    hardScore = 0
    if student.hard_ethics:      hardScore += 14
    if student.hard_study:       hardScore += 20  // Náº·ng nháº¥t
    if student.hard_physical:    hardScore += 12
    if student.hard_volunteer:   hardScore += 19
    if student.hard_integration: hardScore += 15
    // Subtotal: 0-80 (normalize to 0-60% later)
    
    // Component 2: Soft Criteria (Æ¯u tiÃªn)
    softScore = 0
    softScore += (student.soft_ethics_score / 6) * 6
    softScore += (student.soft_study_score / 6) * 6   // Faculty-dependent
    softScore += (student.soft_volunteer_score / 6) * 6
    softScore += (student.soft_integration_score / 6) * 6
    // Subtotal: 0-24
    
    // Component 3: Temporal Bonus
    temporalBonus = 0
    if days_since_submission < 30:
        temporalBonus = +3     // Ná»™p sá»›m
    if update_frequency >= 3:
        temporalBonus += 2     // Cáº­p nháº­t thÆ°á»ng xuyÃªn
    if days_since_update < 7:
        temporalBonus += 2     // Váº«n hoáº¡t Ä‘á»™ng
    // Subtotal: 0-7
    
    // Component 4: Percentile Adjustment
    similarStudents = findKNN(student, k=20)
    studentPercentile = percentile(student.hardScore, similarStudents)
    percentileAdjust = (studentPercentile / 100) * 6
    // Subtotal: 0-6
    
    // Final Aggregation
    totalScore = (hardScore / 80) * 60 +   // Normalize hard to 60%
                 softScore +                // Already 24%
                 temporalBonus +            // 0-7%
                 percentileAdjust           // 0-6%
    
    return min(totalScore, 100)

Example 1: Student with all hard, no soft
â”œâ”€ hardScore = 80 â†’ 60%
â”œâ”€ softScore = 0 â†’ 0%
â”œâ”€ temporal + percentile = 5%
â””â”€ Total = 65% â†’ ALMOST_READY

Example 2: Student with 4/5 hard, good soft
â”œâ”€ hardScore = 68 â†’ 51%
â”œâ”€ softScore = 18 â†’ 18%
â”œâ”€ temporal + percentile = 6%
â””â”€ Total = 75% â†’ ELIGIBLE
```

### 10.2 Success Probability Formula

```
P(success | student) = Î± Ã— P_historical + Î² Ã— P_temporal + Î³ Ã— P_model

where:

P_historical = success rate cá»§a similar students
â”œâ”€ weight Î± = 0.6 (most important)
â””â”€ Based on KNN similarity

P_temporal = adjusted by current year trend
â”œâ”€ weight Î² = 0.2
â”œâ”€ If pass rate increasing â†’ boost probability
â””â”€ Formula: P_historical Ã— (1 + trend_factor)

P_model = predicted probability tá»« ML model
â”œâ”€ weight Î³ = 0.2
â””â”€ model.predict_proba()[target_class]

Final:
â”œâ”€ P(success) = 0.6 Ã— 0.78 + 0.2 Ã— 0.80 + 0.2 Ã— 0.82
â”œâ”€ P(success) = 0.468 + 0.160 + 0.164
â””â”€ P(success) = 0.792 = 79.2%
```

---

## 11. NEXT STEPS & IMPLEMENTATION TIMELINE

### Phase 1: Immediate (Jan-Feb 2025)
- [x] Finalize dataset schema
- [x] Create analytics service
- [ ] Collect & clean historical data
- [ ] EDA & statistics (validation)

### Phase 2: Short-term (Mar-Apr 2025)
- [ ] Train baseline model (Logistic Regression)
- [ ] Hyperparameter tuning (Random Forest)
- [ ] Evaluation & validation
- [ ] Feature importance analysis

### Phase 3: Medium-term (May-Jun 2025)
- [ ] Develop API endpoints
- [ ] Integrate with React frontend
- [ ] Implement caching
- [ ] Add confidence scoring

### Phase 4: Long-term (Jul-Aug 2025)
- [ ] Deploy to production
- [ ] Monitoring & alerting
- [ ] Feedback collection
- [ ] First retraining cycle

---

## 12. SUCCESS CRITERIA

```
Model Performance:
â”œâ”€ Accuracy: â‰¥ 88%
â”œâ”€ Precision: â‰¥ 87%
â”œâ”€ Recall: â‰¥ 85%
â”œâ”€ F1-Score: â‰¥ 0.86
â””â”€ ROC-AUC: â‰¥ 0.90

System Performance:
â”œâ”€ Inference time: < 500ms (p95)
â”œâ”€ Cache hit rate: > 70%
â”œâ”€ API uptime: 99.9%
â””â”€ Error rate: < 0.1%

Business Impact:
â”œâ”€ Student satisfaction: > 80% (survey)
â”œâ”€ Improved eligibility rate: +10% from baseline
â”œâ”€ Admin workload reduction: 30%
â””â”€ Recommendation acceptance rate: > 60%

Data Quality:
â”œâ”€ Missing data: < 5%
â”œâ”€ Outliers: < 2%
â”œâ”€ Data freshness: < 24 hours old
â””â”€ Validation pass rate: > 98%
```

---

**PhÃ¡t triá»ƒn mÃ´ hÃ¬nh lÃ  má»™t quÃ¡ trÃ¬nh liÃªn tá»¥c. Báº¯t Ä‘áº§u nhá», validate, rá»“i má»Ÿ rá»™ng!**
